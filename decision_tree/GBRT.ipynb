{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0141a9e2",
   "metadata": {},
   "source": [
    "This file contains the code used to train GBRT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13fd919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b060cc",
   "metadata": {},
   "source": [
    "Constructing the ROS function to evaluate the model performance based on the definition in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "144b7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_oos(y_true, y_pred):\n",
    "    mask = (~np.isnan(y_true)) & (~np.isnan(y_pred))\n",
    "    y, yp = np.asarray(y_true)[mask], np.asarray(y_pred)[mask]\n",
    "    if len(y) == 0:\n",
    "        return np.nan\n",
    "    rss, tss = np.sum((y - yp) ** 2), np.sum(y ** 2)\n",
    "    return 1 - rss / tss if tss > 0 else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dce8529",
   "metadata": {},
   "source": [
    "Because this model consumes a lot of memory during runtime, a memory detection function has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af082532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_memory_mode(threshold_gb=8):\n",
    "    mem = psutil.virtual_memory()\n",
    "    available_gb = mem.available / (1024**3)\n",
    "    print(f\"Available memory: {available_gb:.2f} GB\")\n",
    "    return available_gb < threshold_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc7a66",
   "metadata": {},
   "source": [
    "gbrt_rolling_validation() implements an annual rolling validation framework for gradient boosting models following the methodology in Gu, Kelly & Xiu (2020).\n",
    "It automatically detects system memory and, if available memory is below a threshold (default 8 GB), switches to HistGradientBoostingRegressor for efficient low-memory computation.\n",
    "\n",
    "This function is optimized for large-scale financial panel data (e.g., monthly firm characteristics and macro variables) and outputs both performance metrics and feature importance visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8338bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbrt_rolling_validation(\n",
    "    data_path=\"data/processed/features.parquet\",\n",
    "    result_dir=\"results\",\n",
    "    train_start=1957,\n",
    "    test_start=1987,\n",
    "    test_end=2016,\n",
    "    target=\"ret_excess_t_plus_1\",\n",
    "    feature_prefixes=(\"c_\", \"m_\", \"sic_\"),\n",
    "    depths=(2, 3, 5),\n",
    "    learning_rates=(0.05, 0.1),\n",
    "    n_trees=(100, 300, 500),\n",
    "    max_features=50,\n",
    "    max_samples=200000,\n",
    "    low_mem_threshold=8\n",
    "):\n",
    "   \n",
    "\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    result_path = os.path.join(result_dir, \"GBRT_rolling_lowmem.parquet\")\n",
    "    param_path = os.path.join(result_dir, \"GBRT_best_params_lowmem.parquet\")\n",
    "\n",
    "\n",
    "    use_hist = low_memory_mode(threshold_gb=low_mem_threshold)\n",
    "\n",
    "\n",
    "    df = pd.read_parquet(data_path)\n",
    "    df[\"month\"] = pd.to_datetime(df[\"month\"], errors=\"coerce\")\n",
    "    df[\"year\"] = df[\"month\"].dt.year\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c.startswith(feature_prefixes)]\n",
    "    df = df.dropna(subset=[target]).reset_index(drop=True)\n",
    "    df[feature_cols] = df[feature_cols].fillna(0).astype(np.float32)\n",
    "    df[target] = df[target].astype(np.float32)\n",
    "\n",
    "    print(f\"✅ Total features used: {len(feature_cols)}\")\n",
    "\n",
    "\n",
    "    results, feat_imps = [], []\n",
    "\n",
    "    for Y in tqdm(range(test_start, test_end + 1), desc=\"Rolling Years\"):\n",
    "        tr_mask = (df[\"year\"] >= train_start) & (df[\"year\"] <= Y - 13)\n",
    "        va_mask = (df[\"year\"] >= Y - 12) & (df[\"year\"] <= Y - 1)\n",
    "        te_mask = (df[\"year\"] == Y)\n",
    "\n",
    "        X_tr, y_tr = df.loc[tr_mask, feature_cols], df.loc[tr_mask, target]\n",
    "        X_va, y_va = df.loc[va_mask, feature_cols], df.loc[va_mask, target]\n",
    "        X_te, y_te = df.loc[te_mask, feature_cols], df.loc[te_mask, target]\n",
    "\n",
    "        if len(X_tr) == 0 or len(X_va) == 0 or len(X_te) == 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        if len(X_tr) > max_samples:\n",
    "            idx = np.random.choice(len(X_tr), max_samples, replace=False)\n",
    "            X_tr, y_tr = X_tr.iloc[idx], y_tr.iloc[idx]\n",
    "\n",
    "        best_r2, best_params = -np.inf, None\n",
    "\n",
    "        for depth in depths:\n",
    "            for lr in learning_rates:\n",
    "                for n_est in n_trees:\n",
    "                    if use_hist:\n",
    "                        model = HistGradientBoostingRegressor(\n",
    "                            max_depth=depth,\n",
    "                            learning_rate=lr,\n",
    "                            max_iter=n_est,\n",
    "                            random_state=42,\n",
    "                        )\n",
    "                    else:\n",
    "                        model = GradientBoostingRegressor(\n",
    "                            n_estimators=n_est,\n",
    "                            learning_rate=lr,\n",
    "                            max_depth=depth,\n",
    "                            max_features=max_features,\n",
    "                            subsample=0.8,\n",
    "                            random_state=42,\n",
    "                        )\n",
    "\n",
    "                    model.fit(X_tr.astype(np.float32), y_tr.astype(np.float32))\n",
    "                    yhat_val = model.predict(X_va.astype(np.float32))\n",
    "                    r2_val = r2_oos(y_va, yhat_val)\n",
    "\n",
    "                    if not np.isnan(r2_val) and r2_val > best_r2:\n",
    "                        best_r2 = r2_val\n",
    "                        best_params = (depth, lr, n_est, model)\n",
    "\n",
    "        if best_params is None:\n",
    "            continue\n",
    "\n",
    "        depth, lr, n_est, model_final = best_params\n",
    "\n",
    "\n",
    "        X_trva = pd.concat([X_tr, X_va]).astype(np.float32)\n",
    "        y_trva = pd.concat([y_tr, y_va]).astype(np.float32)\n",
    "\n",
    "        model_final.fit(X_trva, y_trva)\n",
    "        yhat_train = model_final.predict(X_trva)\n",
    "        yhat_test = model_final.predict(X_te.astype(np.float32))\n",
    "\n",
    "        r2_train = r2_oos(y_trva, yhat_train)\n",
    "        r2_test = r2_oos(y_te, yhat_test)\n",
    "\n",
    "        if hasattr(model_final, \"feature_importances_\"):\n",
    "            feat_imp = model_final.feature_importances_\n",
    "        else:\n",
    "            feat_imp = np.zeros(len(feature_cols))\n",
    "        feat_imps.append(feat_imp)\n",
    "\n",
    "        results.append({\n",
    "            \"year\": Y,\n",
    "            \"depth\": depth,\n",
    "            \"lr\": lr,\n",
    "            \"trees\": n_est,\n",
    "            \"val_r2\": best_r2,\n",
    "            \"train_r2\": r2_train,\n",
    "            \"test_r2\": r2_test,\n",
    "            \"y_true\": y_te.values,\n",
    "            \"y_pred\": yhat_test,\n",
    "        })\n",
    "\n",
    "        print(f\"[{Y}] depth={depth}, lr={lr}, trees={n_est}, \"\n",
    "              f\"ValR²={best_r2:.6f}, TrainR²={r2_train:.6f}, TestR²={r2_test:.6f}\")\n",
    "\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    overall_r2 = r2_oos(\n",
    "        np.concatenate(df_results[\"y_true\"].values),\n",
    "        np.concatenate(df_results[\"y_pred\"].values)\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"[GBRT-LowMem] Overall Out-of-Sample R² = {overall_r2:.6f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    df_results.to_parquet(result_path, index=False)\n",
    "    df_results[[\"year\", \"depth\", \"lr\", \"trees\", \"val_r2\", \"train_r2\", \"test_r2\"]].to_parquet(param_path, index=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(df_results[\"year\"], df_results[\"test_r2\"], marker=\"o\", label=\"Test R²\")\n",
    "    plt.plot(df_results[\"year\"], df_results[\"train_r2\"], marker=\"x\", alpha=0.6, label=\"Train R²\")\n",
    "    plt.axhline(overall_r2, color=\"red\", linestyle=\"--\", alpha=0.7, label=f\"Overall={overall_r2:.4f}\")\n",
    "    plt.title(\"GBRT (Low-Memory) — Annual Out-of-Sample R²\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"R²_oos\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.5, linestyle=\"--\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    avg_imp = np.mean(np.stack(feat_imps), axis=0)\n",
    "    top_idx = np.argsort(avg_imp)[::-1][:15]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(\n",
    "        x=avg_imp[top_idx],\n",
    "        y=np.array(feature_cols)[top_idx],\n",
    "        palette=\"viridis\",\n",
    "        orient=\"h\"\n",
    "    )\n",
    "    plt.title(\"GBRT — Average Feature Importances (Low-Memory Mode)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df_results, overall_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8625a",
   "metadata": {},
   "source": [
    "building main function and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d42f8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
