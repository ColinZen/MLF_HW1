# ğŸŒ² Decision Tree Models

## ğŸ“˜ Overview

This module implements **tree-based models** for empirical asset pricing prediction, following the methodology of
**Gu, Kelly, and Xiu (2020)** â€” *â€œEmpirical Asset Pricing via Machine Learningâ€*.

It includes two major models:

| Model                                        | Description                                                             |
| -------------------------------------------- | ----------------------------------------------------------------------- |
| **Random Forest (RF)**                       | Ensemble of decision trees using bagging and random feature subsampling |
| **Gradient Boosting Regression Tree (GBRT)** | Sequential boosting of shallow trees minimizing squared loss            |

Both models are evaluated under the **rolling-year validation framework** (1957â€“2016) to measure predictive power out-of-sample.

---

## ğŸ§© Files in This Directory

| File                  | Purpose                                                                                      |
| --------------------- | -------------------------------------------------------------------------------------------- |
| `Random_Forest.ipynb` | Implements annual rolling validation for Random Forests with grid search                     |
| `GBRT.ipynb`          | Implements Gradient Boosting Regression Trees (GBRT / HistGBRT) with adaptive memory control |
| `README.md`           | Documentation for this module                                                                |

---

## âš™ï¸ Experimental Design

### 1ï¸âƒ£ Rolling-Year Validation Scheme

For each test year ( Y \in [1987, 2016] ):

| Dataset    | Period      | Purpose                              |
| ---------- | ----------- | ------------------------------------ |
| Training   | 1957 â€“ Yâˆ’13 | Model fitting                        |
| Validation | Yâˆ’12 â€“ Yâˆ’1  | Parameter tuning                     |
| Testing    | Y           | Out-of-sample performance evaluation |

### 2ï¸âƒ£ Evaluation Metric

All models report **Out-of-Sample (R^2_{OOS})** (Gu, Kelly & Xiu, 2020, Eq. 19):

[
R^2_{OOS} = 1 - \frac{\sum_t (y_t - \hat{y}_t)^2}{\sum_t y_t^2}
]

A positive (R^2_{OOS}) indicates predictive skill beyond the unconditional mean model.

---

## ğŸŒ² Random Forest Module

**Notebook:** `Random_Forest.ipynb`

### ğŸ”§ Model Details

* Library: `sklearn.ensemble.RandomForestRegressor`
* Parameters tuned:

  * `max_depth âˆˆ [3, 5, 7]`
  * `n_estimators âˆˆ [100, 300, 500]`
  * `max_features âˆˆ [3, 5, 13]`
* Leaf size fixed at 50, `max_samples=0.7`, `bootstrap=True`

### ğŸ“ˆ Outputs

* **Rolling-year test RÂ² plot**
* **Average feature importance (impurity-based)**
* **Result file:**

  ```
  results/RF_rolling_opt.parquet
  ```

### ğŸ§  Example Output

```python
[1993] depth=5, trees=300, features=5, ValRÂ²=0.0123, TestRÂ²=0.0079
[2004] depth=7, trees=500, features=13, ValRÂ²=0.0215, TestRÂ²=0.0152
```

---

## âš¡ GBRT (Gradient Boosting Regression Tree) Module

**Notebook:** `GBRT.ipynb`

### ğŸ”§ Model Details

* Library:

  * `sklearn.ensemble.GradientBoostingRegressor` (default)
  * `sklearn.ensemble.HistGradientBoostingRegressor` (auto-switch when RAM < 8GB)
* Parameters tuned:

  * `max_depth âˆˆ [2, 3, 5]`
  * `learning_rate âˆˆ [0.05, 0.1]`
  * `n_estimators âˆˆ [100, 300, 500]`
* Subsampling: 0.8
* `max_features = 50`
* Automatic downsampling if `len(X_train) > 200,000`

### ğŸ’¾ Outputs

| File                                      | Description                                     |
| ----------------------------------------- | ----------------------------------------------- |
| `results/GBRT_rolling_lowmem.parquet`     | Annual results of GBRT rolling validation       |
| `results/GBRT_best_params_lowmem.parquet` | Best parameter configuration per year           |
| `results/GBRT_refit_from_params.parquet`  | Re-trained results using stored best parameters |

### ğŸ“Š Plots

* Annual rolling **Train vs Test RÂ²** comparison
* **Average Feature Importances (Top 15)**
* Optional **parameter evolution plots** (depth / learning rate / trees)

---

## ğŸ“Š Visualization Examples

### ğŸ“ˆ Annual RÂ² Plot

```python
plt.plot(df_results["year"], df_results["test_r2"], marker="o", label="Test RÂ²")
plt.axhline(overall_r2, color="red", linestyle="--", alpha=0.7)
plt.title("Random Forest â€” Annual Out-of-Sample RÂ²")
plt.xlabel("Year"); plt.ylabel("RÂ²_oos")
```

### ğŸŒ¿ Feature Importance Plot

```python
sns.barplot(
    x=avg_imp[top_idx],
    y=np.array(feature_cols)[top_idx],
    palette="viridis",
    orient="h"
)
plt.title("GBRT â€” Average Feature Importances")
```

---

## ğŸ”¬ Output Interpretation

| Metric       | Meaning                                  |
| ------------ | ---------------------------------------- |
| `train_r2`   | In-sample fit quality                    |
| `val_r2`     | Validation-year performance (for tuning) |
| `test_r2`    | True out-of-sample predictive power      |
| `overall_r2` | Global performance across all test years |

A **higher and stable annual RÂ²** indicates better generalization across economic cycles.

---

## ğŸ“¦ Dependencies

| Library                 | Function                                     |
| ----------------------- | -------------------------------------------- |
| `pandas`                | Data manipulation & storage                  |
| `numpy`                 | Numerical computation                        |
| `matplotlib`, `seaborn` | Visualization                                |
| `tqdm`                  | Progress tracking for yearly loops           |
| `scikit-learn`          | Machine learning algorithms                  |
| `psutil`                | Dynamic memory detection for low-memory GBRT |
| `pyarrow`               | Efficient Parquet serialization              |

---

## ğŸš€ Usage Guide

### 1ï¸âƒ£ Run Random Forest

```bash
jupyter notebook decision_tree/Random_Forest.ipynb
```

### 2ï¸âƒ£ Run GBRT (auto low-memory mode)

```bash
jupyter notebook decision_tree/GBRT.ipynb
```

### 3ï¸âƒ£ Results Location

```
results/
 â”œâ”€â”€ RF_rolling_opt.parquet
 â”œâ”€â”€ GBRT_rolling_lowmem.parquet
 â”œâ”€â”€ GBRT_best_params_lowmem.parquet
 â””â”€â”€ GBRT_refit_from_params.parquet
```

---

## ğŸ“š Reference

> **Gu, S., Kelly, B., & Xiu, D. (2020)**
> *Empirical Asset Pricing via Machine Learning.*
> *The Review of Financial Studies*, 33(5), 2223â€“2273.

---

## âœï¸ Author

**ColinZen**
Tsinghua University â€” M.Fin (FinTech)
Focus: *Machine Learning & Empirical Asset Pricing*
ğŸ“ Repository: *MLF_HW1 â€” Decision Tree Models Module*

---

