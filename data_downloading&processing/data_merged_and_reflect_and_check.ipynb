{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9533416b",
   "metadata": {},
   "source": [
    "This code integrates CRSP data, datashare data, macroeconomic data, and risk-free interest rate data, and performs lag, monthly cross-sectional normalization, and data breach checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c53930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e0462",
   "metadata": {},
   "source": [
    "First, define a function to combine company characteristic factors with macroeconomic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706829e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_firm_and_macro(firm_path: str, macro_path: str, output_path: str) -> pd.DataFrame:\n",
    "\n",
    "    print(\" Reading datashare.csv and macro_factors.parquet (low-memory mode)...\")\n",
    "\n",
    "    df_macro = pd.read_parquet(macro_path)\n",
    "    df_macro[\"year\"] = df_macro[\"month\"].dt.year\n",
    "    df_macro[\"month_num\"] = df_macro[\"month\"].dt.month\n",
    "\n",
    "    Path(os.path.dirname(output_path)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    chunksize = 200_000  \n",
    "    merged_chunks = []\n",
    "    total_rows = 0\n",
    "\n",
    "    with pd.read_csv(firm_path, engine=\"python\", encoding=\"utf-8\", chunksize=chunksize) as reader:\n",
    "        for chunk in tqdm(reader, desc=\"Merging firm chunks\", unit=\"chunk\"):\n",
    "            total_rows += len(chunk)\n",
    "\n",
    "            chunk[\"DATE\"] = pd.to_datetime(chunk[\"DATE\"].astype(str), format=\"%Y%m%d\", errors=\"coerce\")\n",
    "            chunk[\"year\"] = chunk[\"DATE\"].dt.year\n",
    "            chunk[\"month\"] = chunk[\"DATE\"].dt.month\n",
    "\n",
    "            cols_keep = [\"permno\", \"DATE\", \"sic2\"]\n",
    "            rename_dict = {col: f\"c_{col}\" for col in chunk.columns if col not in cols_keep + [\"year\", \"month\"]}\n",
    "            chunk = chunk.rename(columns=rename_dict)\n",
    "\n",
    "            merged = pd.merge(\n",
    "                chunk, df_macro,\n",
    "                left_on=[\"year\", \"month\"],\n",
    "                right_on=[\"year\", \"month_num\"],\n",
    "                how=\"left\"\n",
    "            ).drop(columns=[\"year\", \"month\", \"month_num\"], errors=\"ignore\")\n",
    "\n",
    "            merged_chunks.append(merged)\n",
    "\n",
    "    print(f\"Concatenating {len(merged_chunks)} chunks ...\")\n",
    "    df_merged = pd.concat(merged_chunks, ignore_index=True)\n",
    "    print(f\"Merge completed ‚Äî total {df_merged.shape[0]:,} rows, {df_merged.shape[1]} columns.\")\n",
    "\n",
    "    df_merged.to_parquet(output_path, index=False)\n",
    "    print(f\" Saved merged dataset to: {output_path}\")\n",
    "\n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c25d1",
   "metadata": {},
   "source": [
    "Next, we define a function to add the risk-free rate, using T-30 data downloaded from the CRSP database as the risk-free rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e19b6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rf_to_macro(parquet_path: str, t_bill_path: str) -> pd.DataFrame:\n",
    "    print(\"Join the risk-free interest rate RF...\")\n",
    "\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    t_bill = pd.read_csv(t_bill_path)\n",
    "    t_bill = t_bill.rename(columns={\"caldt\": \"DATE\", \"t30ret\": \"rf\"})\n",
    "    t_bill[\"DATE\"] = pd.to_datetime(t_bill[\"DATE\"], errors=\"coerce\")\n",
    "\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "    df[\"year\"] = df[\"DATE\"].dt.year\n",
    "    df[\"month\"] = df[\"DATE\"].dt.month\n",
    "\n",
    "    t_bill[\"year\"] = t_bill[\"DATE\"].dt.year\n",
    "    t_bill[\"month\"] = t_bill[\"DATE\"].dt.month\n",
    "\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        t_bill[[\"year\", \"month\", \"rf\"]],\n",
    "        on=[\"year\", \"month\"],\n",
    "        how=\"left\"\n",
    "    ).drop(columns=[\"year\", \"month\"])\n",
    "\n",
    "    if df[\"rf\"].isna().any():\n",
    "        missing = df[\"rf\"].isna().sum()\n",
    "        print(f\"There are {missing:,} missing rf values, which are filled with previous values.\")\n",
    "        df[\"rf\"] = df[\"rf\"].fillna(method=\"ffill\")\n",
    "\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "    print(f\"Written to {parquet_path} | shape={df.shape}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e193f",
   "metadata": {},
   "source": [
    "In this stage, we define the core dependent variable \n",
    "ùë¶\n",
    "y‚Äîthe excess return‚Äîand perform lag adjustments and monthly cross-sectional normalization on firm-level characteristics according to their information release frequency. Following the factor construction framework provided by the original authors, firm characteristics are categorized into annual, quarterly, and monthly variables.\n",
    "\n",
    "Specifically, the excess return \n",
    "ùëü\n",
    "ùë°\n",
    "+\n",
    "1\n",
    "r\n",
    "t+1\n",
    "\t‚Äã\n",
    "\n",
    " is defined as the stock return minus the risk-free rate for the same period, and it is shifted upward by one month (shift(-1)) so that each month‚Äôs feature set corresponds to the next month‚Äôs excess return. Under this structure, variables of different reporting frequencies are aligned according to their respective disclosure lags:\n",
    "\n",
    "Annual variables are lagged by six months (\n",
    "ùëö\n",
    "+\n",
    "6\n",
    "m+6);\n",
    "\n",
    "Quarterly variables are lagged by four months (\n",
    "ùëö\n",
    "+\n",
    "4\n",
    "m+4);\n",
    "\n",
    "Monthly variables are contemporaneous and thus use zero-month lag (\n",
    "ùëö\n",
    "+\n",
    "0\n",
    "m+0).\n",
    "\n",
    "To ensure comparability across firms, we then conduct monthly cross-sectional normalization. Within each month, non-missing observations of every feature are ranked and linearly scaled to the range \n",
    "[\n",
    "‚àí\n",
    "1\n",
    ",\n",
    "1\n",
    "]\n",
    "[‚àí1,1]. Missing values are imputed using the cross-sectional median. To simplify implementation and guarantee that imputed values correspond to the median rank, missing entries are directly set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d02bb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(compustat_macro_path: str, crsp_csv_path: str, out_parquet_path: str) -> pd.DataFrame:\n",
    "\n",
    "    print(\" Building feature data...\")\n",
    "\n",
    "    # === 1) Load data ===\n",
    "    comp = pd.read_parquet(compustat_macro_path)\n",
    "    crsp = pd.read_csv(crsp_csv_path)\n",
    "\n",
    "    comp[\"DATE\"] = pd.to_datetime(comp[\"DATE\"], errors=\"coerce\")\n",
    "    crsp[\"date\"] = pd.to_datetime(crsp[\"date\"], errors=\"coerce\")\n",
    "    crsp = crsp.sort_values([\"permno\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    # === 2) Compute rf on CRSP calendar & ret_excess inside function ===\n",
    "    # rf_calendar from comp (month-end), merged onto CRSP by date\n",
    "    rf_calendar = (\n",
    "        comp.loc[:, [\"DATE\", \"rf\"]]\n",
    "            .drop_duplicates(subset=[\"DATE\"])\n",
    "            .rename(columns={\"DATE\": \"date\"})\n",
    "    )\n",
    "    crsp = crsp.merge(rf_calendar, on=\"date\", how=\"left\")\n",
    "\n",
    "    # robust numeric conversions\n",
    "    crsp[\"ret\"] = pd.to_numeric(crsp.get(\"ret\"), errors=\"coerce\")\n",
    "    crsp[\"dlret\"] = pd.to_numeric(crsp.get(\"dlret\"), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    crsp[\"ret_with_dlret\"] = (1.0 + crsp[\"ret\"].fillna(0.0)) * (1.0 + crsp[\"dlret\"]) - 1.0\n",
    "    crsp[\"ret_excess\"] = crsp[\"ret_with_dlret\"] - crsp[\"rf\"]\n",
    "\n",
    "    # target y = r_{t+1} excess return\n",
    "    crsp[\"ret_excess_t_plus_1\"] = crsp.groupby(\"permno\")[\"ret_excess\"].shift(-1)\n",
    "\n",
    "    # === 3) Define variable groups ===\n",
    "    quarterly_names = [\n",
    "        \"aeavol\", \"cash\", \"chtx\", \"cinvest\", \"ear\", \"roaq\", \"roavol\", \"roeq\",\n",
    "        \"rsup\", \"stdacc\", \"stdcf\", \"ms\", \"nincr\"\n",
    "    ]\n",
    "    monthly_names_c = [\n",
    "        \"baspread\", \"beta\", \"betasq\", \"chmom\", \"dolvol\", \"pricedelay\", \"retvol\",\n",
    "        \"std_dolvol\", \"std_turn\", \"turn\", \"zerotrade\", \"idiovol\", \"ill\", \"indmom\",\n",
    "        \"maxret\", \"mom12m\", \"mom1m\", \"mom36m\", \"mom6m\", \"mvel1\"\n",
    "    ]\n",
    "    monthly_names_m = [\"m_d/p\", \"m_e/p\", \"m_b/m\", \"m_ntis\", \"m_tbl\", \"m_tms\", \"m_dfr\", \"m_svar\"]\n",
    "\n",
    "    all_c_cols = [c for c in comp.columns if c.startswith(\"c_\")]\n",
    "    sic_col = \"sic2\" if \"sic2\" in comp.columns else None\n",
    "\n",
    "    c_quarterly_cols = [f\"c_{x}\" for x in quarterly_names if f\"c_{x}\" in comp.columns]\n",
    "    c_monthly_cols   = [f\"c_{x}\" for x in monthly_names_c if f\"c_{x}\" in comp.columns]\n",
    "    c_annual_cols    = sorted(list(set(all_c_cols) - set(c_quarterly_cols) - set(c_monthly_cols)))\n",
    "\n",
    "    # === 4) Build lag keys & trim right tables to avoid DATE collisions ===\n",
    "    annual = comp[[\"permno\", \"DATE\"] + c_annual_cols + ([sic_col] if sic_col else [])].copy()\n",
    "    annual[\"DATE_LAG_A\"] = annual[\"DATE\"] + pd.DateOffset(months=6)\n",
    "    right_annual = annual[[\"permno\", \"DATE_LAG_A\"] + c_annual_cols + ([sic_col] if sic_col else [])]\n",
    "\n",
    "    quarterly = comp[[\"permno\", \"DATE\"] + c_quarterly_cols].copy()\n",
    "    quarterly[\"DATE_LAG_Q\"] = quarterly[\"DATE\"] + pd.DateOffset(months=4)\n",
    "    right_quarterly = quarterly[[\"permno\", \"DATE_LAG_Q\"] + c_quarterly_cols]\n",
    "\n",
    "    m_firm = comp[[\"permno\", \"DATE\"] + c_monthly_cols].copy()\n",
    "    m_firm[\"DATE_LAG_M\"] = m_firm[\"DATE\"]\n",
    "    right_mfirm = m_firm[[\"permno\", \"DATE_LAG_M\"] + c_monthly_cols]\n",
    "\n",
    "    m_macro = comp[[\"permno\", \"DATE\"] + monthly_names_m].copy()\n",
    "    m_macro[\"DATE_LAG_M\"] = m_macro[\"DATE\"]\n",
    "    right_mmacro = m_macro[[\"permno\", \"DATE_LAG_M\"] + monthly_names_m]\n",
    "\n",
    "    # === 5) Base CRSP panel used for merging ===\n",
    "    crsp_base = crsp[[\"permno\", \"date\", \"ret_with_dlret\", \"ret_excess\", \"ret_excess_t_plus_1\", \"rf\"]].copy()\n",
    "\n",
    "    # === 6) Merge per permno with asof (backward) ===\n",
    "    merged_list = []\n",
    "    for pid, sub in tqdm(crsp_base.groupby(\"permno\"), desc=\"‚è≥ merging\", total=crsp_base[\"permno\"].nunique()):\n",
    "        merged = sub.sort_values(\"date\").copy()\n",
    "\n",
    "        ann_sub = right_annual.query(\"permno == @pid\")\n",
    "        if not ann_sub.empty:\n",
    "            merged = pd.merge_asof(\n",
    "                merged, ann_sub.sort_values(\"DATE_LAG_A\"),\n",
    "                left_on=\"date\", right_on=\"DATE_LAG_A\", by=\"permno\", direction=\"backward\"\n",
    "            )\n",
    "\n",
    "        qtr_sub = right_quarterly.query(\"permno == @pid\")\n",
    "        if not qtr_sub.empty:\n",
    "            merged = pd.merge_asof(\n",
    "                merged, qtr_sub.sort_values(\"DATE_LAG_Q\"),\n",
    "                left_on=\"date\", right_on=\"DATE_LAG_Q\", by=\"permno\", direction=\"backward\"\n",
    "            )\n",
    "\n",
    "        mfirm_sub = right_mfirm.query(\"permno == @pid\")\n",
    "        if not mfirm_sub.empty:\n",
    "            merged = pd.merge_asof(\n",
    "                merged, mfirm_sub.sort_values(\"DATE_LAG_M\"),\n",
    "                left_on=\"date\", right_on=\"DATE_LAG_M\", by=\"permno\", direction=\"backward\"\n",
    "            )\n",
    "\n",
    "        mmacro_sub = right_mmacro.query(\"permno == @pid\")\n",
    "        if not mmacro_sub.empty:\n",
    "            merged = pd.merge_asof(\n",
    "                merged, mmacro_sub.sort_values(\"DATE_LAG_M\"),\n",
    "                left_on=\"date\", right_on=\"DATE_LAG_M\", by=\"permno\", direction=\"backward\"\n",
    "            )\n",
    "\n",
    "        merged_list.append(merged)\n",
    "\n",
    "    df = pd.concat(merged_list, ignore_index=True)\n",
    "    df[\"month\"] = df[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "    # === 7) Rank-based cross-sectional normalization per month (your logic) ===\n",
    "    feature_cols = [c for c in df.columns if c.startswith((\"c_\", \"m_\"))]\n",
    "\n",
    "    def normalize_rank(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        for col in feature_cols:\n",
    "            s = g[col]\n",
    "            if s.isna().all():\n",
    "                continue\n",
    "            valid = s.dropna()\n",
    "            n = len(valid)\n",
    "            if n < 2:\n",
    "                continue\n",
    "            ranks = valid.rank(method=\"first\")\n",
    "            scaled = 2 * (ranks / n) - 1\n",
    "            g.loc[valid.index, col] = scaled\n",
    "            g.loc[s.isna(), col] = 0.0\n",
    "        return g\n",
    "\n",
    "    df = df.groupby(\"month\", group_keys=False).apply(normalize_rank)\n",
    "\n",
    "    # === 8) Save ===\n",
    "    Path(os.path.dirname(out_parquet_path)).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(out_parquet_path, index=False)\n",
    "    print(f\"Features built and saved to {out_parquet_path} | shape={df.shape}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db0d67",
   "metadata": {},
   "source": [
    "Next, we construct a function to check for data leaks. We read the feature and revenue sources and set the information release time for features on different platforms: Annual variables ‚Äì 6 months lag; Quarterly variables ‚Äì 4 months lag; Monthly variables ‚Äì 0 months lag (one month lag relative to r_t+1).\n",
    "\n",
    "Then, we check the data for each stock month by month, ensuring that the release time of all features is ‚â§ the current revenue time.\n",
    "\n",
    "If any feature release time is found to be later than the revenue time, a data leak is determined.\n",
    "\n",
    "We output a text report; if no violations are found, we print \"All good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f47838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakage_check(crsp_csv: str, comp_parquet: str, report_dir: str = \"results\"):\n",
    "    print(\"Running leakage self-check ...\")\n",
    "    REPORT_DIR = Path(report_dir)\n",
    "    REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    comp = pd.read_parquet(comp_parquet)\n",
    "    comp[\"DATE\"] = pd.to_datetime(comp[\"DATE\"], errors=\"coerce\")\n",
    "    crsp = pd.read_csv(crsp_csv)\n",
    "    crsp[\"date\"] = pd.to_datetime(crsp[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    rf_calendar = comp.loc[:, [\"DATE\", \"rf\"]].drop_duplicates().rename(columns={\"DATE\": \"date\"})\n",
    "    crsp = crsp.merge(rf_calendar, on=\"date\", how=\"left\")\n",
    "    crsp[\"ret_excess_t_plus_1\"] = crsp.groupby(\"permno\")[\"ret\"].shift(-1)\n",
    "\n",
    "    quarterly_names = [\n",
    "        'aeavol', \"cash\", \"chtx\", 'cinvest', \"ear\", \"roaq\", \"roavol\", \"roeq\",\n",
    "        \"rsup\", 'stdacc', \"stdcf\", \"ms\", \"nincr\"\n",
    "    ]\n",
    "    monthly_names_c = [\n",
    "        'baspread', \"beta\", \"betasq\", \"chmom\", \"dolvol\", \"pricedelay\", \"retvol\",\n",
    "        'std_dolvol', \"std_turn\", \"turn\", \"zerotrade\", \"idiovol\", \"ill\", \"indmom\",\n",
    "        'maxret', \"mom12m\", \"mom1m\", \"mom36m\", \"mom6m\", \"mvel1\"\n",
    "    ]\n",
    "    all_c_cols = [c for c in comp.columns if str(c).startswith(\"c_\")]\n",
    "    c_annual_cols = sorted(list(set(all_c_cols) - set([f\"c_{n}\" for n in quarterly_names]) - set([f\"c_{n}\" for n in monthly_names_c])))\n",
    "\n",
    "    annual = comp[[\"permno\", \"DATE\"] + c_annual_cols].copy()\n",
    "    annual[\"RELEASE_A\"] = annual[\"DATE\"] + pd.DateOffset(months=6)\n",
    "    qtr = comp[[\"permno\", \"DATE\"] + [f\"c_{n}\" for n in quarterly_names]].copy()\n",
    "    qtr[\"RELEASE_Q\"] = qtr[\"DATE\"] + pd.DateOffset(months=4)\n",
    "    m_firm = comp[[\"permno\", \"DATE\"] + [f\"c_{n}\" for n in monthly_names_c]].copy()\n",
    "    m_firm[\"RELEASE_M\"] = m_firm[\"DATE\"]\n",
    "\n",
    "    violations = {\"annual\": 0, \"quarterly\": 0, \"monthly\": 0}\n",
    "    checked_rows = 0\n",
    "\n",
    "    for pid in tqdm(crsp[\"permno\"].unique(), desc=\"checking\"):\n",
    "        sub = crsp[crsp[\"permno\"] == pid][[\"permno\", \"date\", \"ret_excess_t_plus_1\"]]\n",
    "        ann_sub, qtr_sub, m_sub = annual.query(\"permno == @pid\"), qtr.query(\"permno == @pid\"), m_firm.query(\"permno == @pid\")\n",
    "\n",
    "        merged = sub.sort_values(\"date\")\n",
    "        for dset, col, rule in [(ann_sub, \"RELEASE_A\", \"annual\"), (qtr_sub, \"RELEASE_Q\", \"quarterly\"), (m_sub, \"RELEASE_M\", \"monthly\")]:\n",
    "            if not dset.empty:\n",
    "                merged = pd.merge_asof(merged, dset.sort_values(col), left_on=\"date\", right_on=col, by=\"permno\", direction=\"backward\")\n",
    "                violations[rule] += int(((merged[col].notna()) & (merged[col] > merged[\"date\"])).sum())\n",
    "        checked_rows += len(merged)\n",
    "\n",
    "    report_path = Path(REPORT_DIR) / \"leakage_report.txt\"\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Leakage Check Report\\nTotal rows: {checked_rows}\\n\")\n",
    "        for k, v in violations.items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "    print(f\" Leakage report saved: {report_path}\")\n",
    "    if any(v > 0 for v in violations.values()):\n",
    "        raise AssertionError(f\"Leakage check failed: {violations}\")\n",
    "    else:\n",
    "        print(\"All good: no look-ahead leakage detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bdacdf",
   "metadata": {},
   "source": [
    "Construct the main function and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48df6853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reading datashare.csv and macro_factors.parquet (low-memory mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging firm chunks: 21chunk [07:39, 21.88s/chunk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating 21 chunks ...\n",
      "Merge completed ‚Äî total 4,117,300 rows, 107 columns.\n",
      " Saved merged dataset to: ../data/processed/datashare_with_macro.parquet\n",
      "Join the risk-free interest rate RF...\n",
      "There are 355,161 missing rf values, which are filled with previous values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zheng\\AppData\\Local\\Temp\\ipykernel_45316\\629236407.py:26: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[\"rf\"] = df[\"rf\"].fillna(method=\"ffill\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to ../data/processed/datashare_with_macro.parquet | shape=(4117300, 108)\n",
      " Building feature data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚è≥ merging: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32791/32791 [2:03:53<00:00,  4.41it/s]\n",
      "C:\\Users\\zheng\\AppData\\Local\\Temp\\ipykernel_45316\\3982711931.py:127: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(\"month\", group_keys=False).apply(normalize_rank)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features built and saved to ../data/processed/features.parquet | shape=(4353483, 114)\n",
      "Running leakage self-check ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checking: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32791/32791 [1:50:47<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Leakage report saved: results\\leakage_report.txt\n",
      "All good: no look-ahead leakage detected.\n",
      "The entire process is complete; features.parquet has been generated and passed the leakage check.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    FIRM_CSV = \"../data/raw/datashare.csv\"\n",
    "    MACRO_PARQUET = \"../data/raw/macro_factors.parquet\"\n",
    "    TBILL_CSV = \"../data/raw/t_bill.csv\"\n",
    "    CRSP_CSV = \"../data/raw/crsp_monthly_1957_2021.csv\"\n",
    "    MERGED_PARQUET = \"../data/processed/datashare_with_macro.parquet\"\n",
    "    FEATURES_PARQUET = \"../data/processed/features.parquet\"\n",
    "\n",
    "    Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"results\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    merge_firm_and_macro(FIRM_CSV, MACRO_PARQUET, MERGED_PARQUET)\n",
    "    add_rf_to_macro(MERGED_PARQUET, TBILL_CSV)\n",
    "    build_features(MERGED_PARQUET, CRSP_CSV, FEATURES_PARQUET)\n",
    "    leakage_check(CRSP_CSV, MERGED_PARQUET, \"results\")\n",
    "\n",
    "    print(\"The entire process is complete; features.parquet has been generated and passed the leakage check.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9842d300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
